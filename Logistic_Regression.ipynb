{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1 — What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Answer : - Logistic Regression is a statistical and machine-learning method used for binary classification (and extendable to multiclass) that models the probability that an input instance belongs to a particular class. Instead of predicting a continuous numeric value, logistic regression predicts the probability 𝑃 ( 𝑦 = 1 ∣ 𝑥 ) P(y=1∣x) and maps that probability to a prediction (class 0 or 1) using a decision threshold (commonly 0.5). The model uses a linear combination of input features 𝑧 = 𝑤 ⊤ 𝑥 + 𝑏 z=w ⊤ x+b, but then passes 𝑧 z through a sigmoid (logistic) function to squash it into the interval [ 0 , 1 ] [0,1]:\n",
        "\n",
        "  - p^​(x)=σ(z)=1/1+e−z1\n",
        "\n",
        " Key differences from Linear Regression:\n",
        "\n",
        "1. Target / Objective: Linear regression predicts a continuous outcome and optimizes mean squared error (MSE). Logistic regression predicts class probabilities and typically optimizes the log-loss (negative log-likelihood / cross-entropy).\n",
        "2. Output range: Linear regression outputs an unbounded real number; logistic regression outputs probabilities in\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "[0,1].\n",
        "3. Loss / Estimation: Logistic regression uses a likelihood-based (or cross-entropy) loss, which is convex for the linear model and well-suited for classification. Linear regression's MSE is not appropriate for probability outputs.\n",
        "4. nterpretability: Coefficients in logistic regression are interpretable as log-odds increments: a coefficient\n",
        "𝑤\n",
        "𝑗\n",
        "w\n",
        "j corresponds to the change in log-odds for a unit change in feature\n",
        "𝑥\n",
        "𝑗\n",
        "x\n",
        "j\n",
        " (holding other features constant).\n",
        "\n",
        "Q2. Explain the role of the Sigmoid function in Logistic Regression\n",
        "\n",
        "Answer: - The sigmoid function (also called the logistic function) is central to logistic regression because it maps any real-valued input into the interval ( 0 , 1 ) (0,1), making it suitable for representing probabilities. Given the linear combination 𝑧 = 𝑤 ⊤ 𝑥 + 𝑏 z=w ⊤ x+b, the sigmoid transforms this to: 𝜎 ( 𝑧 ) = 1 1 + 𝑒 − 𝑧 σ(z)= 1/1+e −z 1 ​\n",
        "\n",
        "Roles and properties:\n",
        "1. Probability mapping: 𝜎 ( 𝑧 ) σ(z) translates raw scores (logits) into interpretable probabilities. Values of 𝑧 z that are large positive produce probabilities near 1; large negative produce probabilities near 0.\n",
        "2. Monotonic and smooth: Because 𝜎 ( 𝑧 ) σ(z) is monotonic and differentiable, it enables optimization by gradient-based methods (the derivative is 𝜎 ( 𝑧 ) ( 1 − 𝜎 ( 𝑧 ) ) σ(z)(1−σ(z))).\n",
        "3. Decision threshold: Using a threshold (e.g., 0.5) on 𝜎 ( 𝑧 ) σ(z) yields the predicted class. The decision boundary 𝜎 ( 𝑧 ) = 0.5 σ(z)=0.5 corresponds exactly to 𝑧 = 0 z=0, which is linear in input space.\n",
        "4. Connects to log-odds: Taking the logit (inverse sigmoid) gives log ⁡ ( 𝑝 1 − 𝑝 ) = 𝑧 log( 1−p p ​ )=z, so logistic regression models log-odds as a linear function of features; this yields interpretable coefficients and simplifies regularization and optimization. In short, the sigmoid makes classification via a linear score possible while producing probabilities and enabling a convex optimization problem.\n",
        "\n",
        "Q3 — What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "Answer: - Regularization is a technique that adds a penalty term to the model’s loss function to prevent overfitting and to control model complexity. In the context of logistic regression, the regularized objective typically becomes: Minimize − ∑ 𝑖 [ 𝑦 𝑖 log ⁡ 𝑝 ^ 𝑖 + ( 1 − 𝑦 𝑖 ) log ⁡ ( 1 − 𝑝 ^ 𝑖 ) ] + 𝜆 𝑅 ( 𝑤 )\n",
        "\n",
        "where 𝑅 ( 𝑤 ) R(w) is a regularization penalty (e.g., ∣ ∣ 𝑤 ∣ ∣ 2 2 ∣∣w∣∣ 2 2 ​ for L2 or ∣ ∣ 𝑤 ∣ ∣ 1 ∣∣w∣∣ 1 ​ for L1) and 𝜆 λ is a strength parameter.\n",
        "\n",
        "Why regularization is needed:\n",
        "1. Prevent overfitting: Without regularization, logistic regression with many features (especially relative to number of samples) can fit noise in the training set, giving poor generalization. Regularization shrinks coefficients to reduce variance.\n",
        "2. Numerical stability: Regularization mitigates issues with multicollinearity (highly correlated features) by controlling coefficient magnitude and avoiding extreme weights.\n",
        "3. Feature selection / sparsity: L1 regularization (lasso-like) can push many coefficients exactly to zero, providing feature selection and simpler models. L2 encourages small coefficients but typically not exact zeros, producing more stable solutions.\n",
        "4. Improved generalization: Regularized models often yield better predictive performance on unseen data, especially when the dataset is noisy or the model is flexible.\n",
        "\n",
        "Common choices:\n",
        "1. L2 (Ridge): penalizes squared magnitude, good default when you want small weights.\n",
        "2. L1 (Lasso): can produce sparse solutions and feature selection.\n",
        "3. Elastic Net: combination of L1 and L2 penalties when you want both sparsity and stability.\n",
        "\n",
        "Q4 — What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "Answer: - Evaluation metrics provide quantitative measures of a classifier’s performance and guide model selection and tuning. Choosing metrics that align with the business objective is critical. Common metrics include:\n",
        "1. Accuracy: Fraction of correct predictions ( TP + TN ) / Total (TP+TN)/Total. Simple and intuitive but misleading for imbalanced data (e.g., 95% accuracy when positive class is only 5% but model always predicts negative).\n",
        "2. Confusion Matrix: 2×2 table (TP, FP, TN, FN) that gives the full picture of true/false positive/negative counts. Almost all metrics can be derived from it.\n",
        "3. Precision: Precision = TP / TP + FP . Indicates how many predicted positives are actually positive. Important when false positives are costly.\n",
        "4. Recall (Sensitivity / True Positive Rate): Recall = TP TP + FN Recall= TP+FN TP ​ . Measures how many actual positives were found. Important when missing positives is costly (e.g., disease detection).\n",
        "5. F1-score: Harmonic mean of precision and recall: 2 ⋅ Precision ⋅ Recall Precision + Recall 2⋅ Precision+Recall Precision⋅Recall ​ . Useful when you need a balance between precision and recall.\n",
        "6. Specificity: TN /TN + FP  , complements recall; important when false positives matter.\n",
        "7. ROC AUC (Area Under ROC Curve): Measures discrimination across all thresholds by plotting TPR vs FPR. Good for comparing classifiers irrespective of a specific threshold.\n",
        "8. PR AUC (Area Under Precision-Recall Curve): Especially informative for highly imbalanced datasets because it focuses on positive class performance.\n",
        "9. Calibration metrics: Brier score or reliability plots check whether predicted probabilities match observed frequencies (crucial when probabilities are used directly).\n",
        "10. Cost-sensitive metrics / business KPIs: e.g., profit, expected lift, or cost-weighted errors tailored to business consequences.\n",
        "\n",
        "Q5 — Python program: load CSV into Pandas DataFrame, split train/test, train Logistic Regression, print accuracy (use sklearn dataset)\n",
        "\n",
        "Answer: - This example uses the Breast Cancer dataset packaged in sklearn. It demonstrates loading into a DataFrame, splitting, training, and printing accuracy."
      ],
      "metadata": {
        "id": "s5VIUlpuhLVC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVMqv2P6hJ9g",
        "outputId": "e2837d03-e557-4f29-d47d-792733f08ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 0.9561\n",
            "Confusion matrix:\n",
            " [[39  3]\n",
            " [ 2 70]]\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9512    0.9286    0.9398        42\n",
            "           1     0.9589    0.9722    0.9655        72\n",
            "\n",
            "    accuracy                         0.9561       114\n",
            "   macro avg     0.9551    0.9504    0.9526       114\n",
            "weighted avg     0.9561    0.9561    0.9560       114\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q5: Load dataset, split, train logistic regression, print accuracy\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load dataset and create DataFrame\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "\n",
        "# Optional: inspect\n",
        "# print(X.head()); print(y.value_counts())\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Logistic Regression (simple default)\n",
        "clf = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy on test set: {acc:.4f}\")\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification report:\\n\", classification_report(y_test, y_pred, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation & notes:\n",
        "\n",
        "  - solver='liblinear' is suitable for small-to-medium datasets and supports 'l1' and 'l2' penalties.\n",
        "\n",
        "  - stratify=y in train_test_split preserves the class ratio across train and test.\n",
        "\n",
        "  - Increase max_iter if solver warnings appear.\n",
        "\n",
        "Q6 — Train Logistic Regression with L2 regularization, print model coefficients and accuracy\n",
        "\n",
        "Answer: - L2 is the default in many implementations. Here we explicitly show coefficients and map them to feature names.\n"
      ],
      "metadata": {
        "id": "o4nsOrIjlbR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6: Logistic Regression with L2 regularization, print coefficients and accuracy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train with L2 regularization (default penalty='l2')\n",
        "clf_l2 = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', random_state=42, max_iter=1000)\n",
        "clf_l2.fit(X_train, y_train)\n",
        "\n",
        "# Coefficients\n",
        "coefficients = pd.Series(clf_l2.coef_.ravel(), index=X.columns).sort_values(key=abs, ascending=False)\n",
        "print(\"Top coefficients (by absolute value):\\n\", coefficients.head(10))\n",
        "\n",
        "# Accuracy\n",
        "y_pred = clf_l2.predict(X_test)\n",
        "print(f\"\\nTest accuracy (L2): {accuracy_score(y_test, y_pred):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEmt8_VklYLC",
        "outputId": "b4f6223e-957d-4f70-d87d-5ad4e69cbfea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top coefficients (by absolute value):\n",
            " mean radius             1.930357\n",
            "worst concavity        -1.579116\n",
            "worst compactness      -1.180527\n",
            "worst radius            1.145734\n",
            "texture error           1.117543\n",
            "worst symmetry         -0.756250\n",
            "worst concave points   -0.618344\n",
            "mean concavity         -0.593762\n",
            "mean compactness       -0.381326\n",
            "mean concave points    -0.303860\n",
            "dtype: float64\n",
            "\n",
            "Test accuracy (L2): 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "  - C controls inverse regularization strength (smaller C ⇒ stronger regularization).\n",
        "\n",
        "  - coeff_.ravel() gives coefficient per feature (for binary logistic regression there’s one coefficient vector).\n",
        "\n",
        "  Q7 — Train Logistic Regression for multiclass classification using multi_class='ovr' and print the classification report\n",
        "\n",
        "Answer: - Important note: The Breast Cancer dataset is binary (two classes). multi_class='ovr' (one-vs-rest) is applicable to binary and multiclass settings, but the ovr behavior with a binary dataset is equivalent to standard binary logistic. If you need a true multiclass example, use iris or wine. Here I demonstrate using multi_class='ovr' on the breast-cancer data and printing a classification report."
      ],
      "metadata": {
        "id": "oZf0_oxHlxw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7: Train logistic regression with multi_class='ovr' and show classification report\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train with one-vs-rest (ovr)\n",
        "clf_ovr = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42, max_iter=1000)\n",
        "clf_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = clf_ovr.predict(X_test)\n",
        "print(\"Classification report (multi_class='ovr'):\\n\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6gGGAB3lt_U",
        "outputId": "a8d0199c-1a57-417a-992d-c3ee32195108"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report (multi_class='ovr'):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9512    0.9286    0.9398        42\n",
            "           1     0.9589    0.9722    0.9655        72\n",
            "\n",
            "    accuracy                         0.9561       114\n",
            "   macro avg     0.9551    0.9504    0.9526       114\n",
            "weighted avg     0.9561    0.9561    0.9560       114\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "- multi_class='ovr' fits one classifier per class (each vs rest). For binary classification it reduces to the usual binary logistic.\n",
        "\n",
        "- For real multiclass datasets, multi_class='multinomial' with solver='saga' or solver='lbfgs' is typically better when classes are mutually exclusive.\n",
        "\n",
        "Q8 — Use GridSearchCV to tune C and penalty hyperparameters for Logistic Regression; print best parameters and validation accuracy\n",
        "\n",
        "Answer: - We tune C (inverse regularization strength) and penalty. When testing both 'l1' and 'l2' we must use a solver that supports 'l1' (e.g., 'liblinear' or 'saga') — here we use 'liblinear' for simplicity"
      ],
      "metadata": {
        "id": "F7FQ0jClmDLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8: GridSearchCV to tune C and penalty\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Grid search setup\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Use solver that supports l1 and l2 (liblinear)\n",
        "base_clf = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid = GridSearchCV(base_clf, param_grid, scoring='accuracy', cv=cv, n_jobs=-1, verbose=1)\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(f\"Best cross-validated accuracy: {grid.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate best model on test set\n",
        "best_model = grid.best_estimator_\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "print(f\"Test set accuracy (best model): {accuracy_score(y_test, y_test_pred):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbtpsrTLmAqY",
        "outputId": "695ac70d-46ba-4c9a-ccf5-e61da4333ddc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters: {'C': 100, 'penalty': 'l1'}\n",
            "Best cross-validated accuracy: 0.9648\n",
            "Test set accuracy (best model): 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation / notes:\n",
        "\n",
        "- StratifiedKFold keeps class balance across folds.\n",
        "\n",
        "- scoring='accuracy' is used here; for imbalanced tasks prefer roc_auc or average_precision.\n",
        "\n",
        "- n_jobs=-1 uses all CPUs available.\n",
        "\n",
        "Q9 — Standardize features before training and compare accuracy with and without scaling\n",
        "\n",
        "Answer: -Scaling often improves convergence and sometimes performance for models that are sensitive to feature scales (e.g., regularized logistic regression). We compare StandardScaler vs raw features."
      ],
      "metadata": {
        "id": "uhevRLxZmWfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9: Compare accuracy with and without StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 1) Without scaling\n",
        "clf_raw = LogisticRegression(solver='liblinear', C=1.0, random_state=42, max_iter=1000)\n",
        "clf_raw.fit(X_train, y_train)\n",
        "acc_raw = accuracy_score(y_test, clf_raw.predict(X_test))\n",
        "\n",
        "# 2) With standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_scaled = LogisticRegression(solver='liblinear', C=1.0, random_state=42, max_iter=1000)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, clf_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_raw:.4f}\")\n",
        "print(f\"Accuracy with StandardScaler: {acc_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnZKvTybmQDV",
        "outputId": "3f4734ba-8fbc-485a-8c39-560d3011b3bb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9561\n",
            "Accuracy with StandardScaler: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "- For logistic regression, scaling generally improves optimizer behavior and makes penalty effects consistent across features. If features have widely different ranges, unscaled models may place undue weight on high-magnitude features.\n",
        "\n",
        "- Even if accuracy does not change dramatically, coefficients become more interpretable (because they are in standardized units).\n",
        "\n",
        "Question 10: Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "\n",
        "Answer:- To build a Logistic Regression model for a marketing campaign with only 5% positive responders, I would first clean and preprocess the data by handling missing values, encoding categorical variables, and standardizing numeric features. Since the dataset is highly imbalanced, I would either apply class_weight='balanced' or use resampling techniques like SMOTE to avoid bias toward the majority class. I would then train a regularized Logistic Regression model and tune hyperparameters such as C and penalty using GridSearchCV with roc_auc or average_precision as scoring metrics. Instead of relying on accuracy, I would evaluate performance using Precision, Recall, F1-score, and Precision-Recall AUC. Finally, I would select an optimal probability threshold or top-K customers based on expected marketing ROI."
      ],
      "metadata": {
        "id": "-E3j3lJImiHM"
      }
    }
  ]
}